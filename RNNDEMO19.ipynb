{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOM92shirs1O9LarXqXGymN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MohamedElsayed002/DeepLearning_IBM/blob/master/RNNDEMO19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN Demo\n",
        "\n",
        "## Using RNNs to classify sentiment on IMDB data\n",
        "\n",
        "For this exercies we will train a \"vanialla\" RNN to predict the sentiment on IMDB reviews. our data consists of 25000 training sequences and 25000 test sequences the outcome is binary (postive/negative) and both outcomes are equally represented in both the trainnig and the test set.\n",
        "\n",
        "Keras provides a convenient interafce to load the data immediately encode the words into integers (based on the most common words). this will save us alot of re drudgery that is usually involved when working with raw text\n",
        "\n",
        "We will walk through the preparation of the data and the building an RNN model. Then it will be your turn to build your own models (and prepare the data how you see fit)"
      ],
      "metadata": {
        "id": "9Gb0WBAXgK6k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87NQU0hFgEbE"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.utils import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import SimpleRNN\n",
        "from keras.datasets import imdb\n",
        "from keras import initializers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 20000 # This is used in the loading the data,\n",
        "maxLen = 30 # maximum length of a sequence\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "MD1g2VYDhH2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in the data. the function automatically tokenizes the text into distinct integeres\n",
        "(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train),'train sequences')\n",
        "print(len(x_test),'test sequences')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GpGNDlxi03m",
        "outputId": "8948ede2-0f1c-4c1e-ee29-dcedf6de173f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 1s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This pads (or truncates) the sequences so that they are of the maximum length\n",
        "x_train = pad_sequences(x_train, maxlen=maxLen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxLen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcEV2WpejMai",
        "outputId": "63f4c751-1440-46a1-b72d-435146e8be1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (25000, 30)\n",
            "x_test shape: (25000, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[123,:]  #Here's what an example sequence looks like"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTzB04fWjiAS",
        "outputId": "4b505850-76f5-47a0-c774-8323f735bdf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  219,   141,    35,   221,   956,    54,    13,    16,    11,\n",
              "        2714,    61,   322,   423,    12,    38,    76,    59,  1803,\n",
              "          72,     8, 10508,    23,     5,   967,    12,    38,    85,\n",
              "          62,   358,    99], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras layers for (Vanilla) RNNs\n",
        "\n",
        "In this exercise, we will not use pre-trained word vectors.  Rather we will learn an embedding as part of the Neural Network.  This is represented by the Embedding Layer below.\n",
        "\n",
        "### Embedding Layer\n",
        "`keras.layers.embeddings.Embedding(input_dim, output_dim, embeddings_initializer='uniform', embeddings_regularizer=None, activity_regularizer=None, embeddings_constraint=None, mask_zero=False, input_length=None)`\n",
        "\n",
        "- This layer maps each integer into a distinct (dense) word vector of length `output_dim`.\n",
        "- Can think of this as learning a word vector embedding \"on the fly\" rather than using an existing mapping (like GloVe)\n",
        "- The `input_dim` should be the size of the vocabulary.\n",
        "- The `input_length` specifies the length of the sequences that the network expects.\n",
        "\n",
        "### SimpleRNN Layer\n",
        "`keras.layers.recurrent.SimpleRNN(units, activation='tanh', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)`\n",
        "\n",
        "- This is the basic RNN, where the output is also fed back as the \"hidden state\" to the next iteration.\n",
        "- The parameter `units` gives the dimensionality of the output (and therefore the hidden state).  Note that typically there will be another layer after the RNN mapping the (RNN) output to the network output.  So we should think of this value as the desired dimensionality of the hidden state and not necessarily the desired output of the network.\n",
        "- Recall that there are two sets of weights, one for the \"recurrent\" phase and the other for the \"kernel\" phase.  These can be configured separately in terms of their initialization, regularization, etc.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oBDOW60pj0Ud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Let's build a RNN\n",
        "\n",
        "rnn_hidden_dim = 5\n",
        "word_embedding_dim = 50\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(Embedding(max_features,word_embedding_dim)) # This layer takes each integer in the sequence and embeds it in a 50-dimensional vector.\n",
        "model_rnn.add(SimpleRNN(rnn_hidden_dim,\n",
        "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
        "                    recurrent_initializer=initializers.Identity(gain=1.0),\n",
        "                    activation='relu',\n",
        "                    input_shape=x_train.shape[1:]))\n",
        "\n",
        "model_rnn.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "eJK8r4zgjy8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Note that most of the parameters come from the embedding layer\n",
        "model_rnn.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ised9azllC7t",
        "outputId": "3c482d92-51a3-4049-ea61-32f25a012b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 50)          1000000   \n",
            "                                                                 \n",
            " simple_rnn (SimpleRNN)      (None, 5)                 280       \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 6         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1000286 (3.82 MB)\n",
            "Trainable params: 1000286 (3.82 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rmsprop = keras.optimizers.RMSprop(learning_rate = .0001)\n",
        "\n",
        "model_rnn.compile(loss='binary_crossentropy',\n",
        "                  optimizer=rmsprop,\n",
        "                  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "puBsY0zvlFR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn.fit(x_train,y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=10,\n",
        "              validation_data=(x_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nM-wQs6klVIk",
        "outputId": "86a5887e-b33f-4df4-e897-1ecaa9922f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 26s 31ms/step - loss: 0.6777 - accuracy: 0.5908 - val_loss: 0.6439 - val_accuracy: 0.6513\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 22s 28ms/step - loss: 0.6033 - accuracy: 0.6812 - val_loss: 0.5775 - val_accuracy: 0.7002\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 16s 20ms/step - loss: 0.5380 - accuracy: 0.7309 - val_loss: 0.5388 - val_accuracy: 0.7267\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 16s 21ms/step - loss: 0.4931 - accuracy: 0.7622 - val_loss: 0.5147 - val_accuracy: 0.7438\n",
            "Epoch 5/10\n",
            "780/782 [============================>.] - ETA: 0s - loss: 0.4601 - accuracy: 0.7837Epoch 6/10\n",
            "782/782 [==============================] - 17s 22ms/step - loss: 0.4349 - accuracy: 0.8011 - val_loss: 0.4801 - val_accuracy: 0.7659\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.4152 - accuracy: 0.8134 - val_loss: 0.4725 - val_accuracy: 0.7709\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 17s 22ms/step - loss: 0.3996 - accuracy: 0.8182 - val_loss: 0.4651 - val_accuracy: 0.7747\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 14s 18ms/step - loss: 0.3874 - accuracy: 0.8290 - val_loss: 0.4627 - val_accuracy: 0.7770\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 15s 19ms/step - loss: 0.3771 - accuracy: 0.8339 - val_loss: 0.4555 - val_accuracy: 0.7809\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b4e71786b90>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score , acc = model_rnn.evaluate(x_test,y_test,\n",
        "                                 batch_size=batch_size)\n",
        "\n",
        "print('Test score:', score)\n",
        "print('Test accuracy,', acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9-PsiWmldEr",
        "outputId": "96c69877-23ab-46e1-92ec-026f5a9b711a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 3s 3ms/step - loss: 0.4555 - accuracy: 0.7809\n",
            "Test score: 0.4554561674594879\n",
            "Test accuracy, 0.7809200286865234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercies\n",
        "\n",
        "In this ecerices, we will illustrate:\n",
        "\n",
        "- Preparing the data to use sequences of length 80 rather than length 30.  Does it improve the performance?\n",
        "- Trying different values of the \"max_features\".  Does this  improve the performance?\n",
        "- Trying smaller and larger sizes of the RNN hidden dimension.  How does it affect the model performance?  How does it affect the run time?\n"
      ],
      "metadata": {
        "id": "jvDjSzXhmrf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 20000  # This is used in loading the data, picks the most common (max_features) words\n",
        "maxlen = 80  # maximum length of a sequence - truncate after this\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "aootSW_JloPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_hidden_dim = 5\n",
        "word_embedding_dim = 50\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence\n",
        "model_rnn.add(SimpleRNN(rnn_hidden_dim,\n",
        "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
        "                    recurrent_initializer=initializers.Identity(gain=1.0),\n",
        "                    activation='relu',\n",
        "                    input_shape=x_train.shape[1:]))\n",
        "\n",
        "model_rnn.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "q8JHtC4lm27Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmsprop = keras.optimizers.RMSprop(learning_rate = .0001)\n",
        "\n",
        "model_rnn.compile(loss='binary_crossentropy',\n",
        "              optimizer=rmsprop,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "BJm8eFzEm4Rs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=10,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwoHIy7wm5hx",
        "outputId": "d445f655-c9d9-4432-dbdb-2b7715b1826f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 30s 37ms/step - loss: 0.6713 - accuracy: 0.5907 - val_loss: 0.6156 - val_accuracy: 0.6819\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 27s 34ms/step - loss: 0.5423 - accuracy: 0.7329 - val_loss: 0.5205 - val_accuracy: 0.7466\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.4541 - accuracy: 0.7897 - val_loss: 0.4749 - val_accuracy: 0.7693\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 27s 34ms/step - loss: 0.4025 - accuracy: 0.8219 - val_loss: 0.4385 - val_accuracy: 0.7950\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 26s 33ms/step - loss: 0.3668 - accuracy: 0.8408 - val_loss: 0.4188 - val_accuracy: 0.8065\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 31s 39ms/step - loss: 0.3386 - accuracy: 0.8547 - val_loss: 0.4060 - val_accuracy: 0.8125\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.3163 - accuracy: 0.8669 - val_loss: 0.3965 - val_accuracy: 0.8193\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.2986 - accuracy: 0.8764 - val_loss: 0.4015 - val_accuracy: 0.8165\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.2841 - accuracy: 0.8835 - val_loss: 0.4149 - val_accuracy: 0.8148\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 0.2724 - accuracy: 0.8892 - val_loss: 0.3746 - val_accuracy: 0.8331\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b4e78932bf0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 5000  # This is used in loading the data, picks the most common (max_features) words\n",
        "maxlen = 80  # maximum length of a sequence - truncate after this\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = pad_sequences(x_test, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "KY-LRf9mm6r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn_hidden_dim = 5\n",
        "word_embedding_dim = 20\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(Embedding(max_features, word_embedding_dim))  #This layer takes each integer in the sequence\n",
        "model_rnn.add(SimpleRNN(rnn_hidden_dim,\n",
        "                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n",
        "                    recurrent_initializer=initializers.Identity(gain=1.0),\n",
        "                    activation='relu',\n",
        "                    input_shape=x_train.shape[1:]))\n",
        "\n",
        "model_rnn.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "ycaLIvj-m8Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmsprop = keras.optimizers.RMSprop(learning_rate = .0001)\n",
        "\n",
        "model_rnn.compile(loss='binary_crossentropy',\n",
        "              optimizer=rmsprop,\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "wUSAH2Ekm9z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_rnn.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=10,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNU4jgaTm_Rd",
        "outputId": "4181434e-576d-4eb8-e937-16832de92964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 23s 28ms/step - loss: 0.6548 - accuracy: 0.6018 - val_loss: 0.5956 - val_accuracy: 0.6803\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.5219 - accuracy: 0.7419 - val_loss: 0.4903 - val_accuracy: 0.7693\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.4420 - accuracy: 0.7991 - val_loss: 0.4479 - val_accuracy: 0.7841\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.3978 - accuracy: 0.8237 - val_loss: 0.4222 - val_accuracy: 0.8046\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 18s 24ms/step - loss: 0.3729 - accuracy: 0.8350 - val_loss: 0.4018 - val_accuracy: 0.8162\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 20s 26ms/step - loss: 0.3549 - accuracy: 0.8449 - val_loss: 0.4080 - val_accuracy: 0.8117\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 18s 24ms/step - loss: 0.3410 - accuracy: 0.8532 - val_loss: 0.3880 - val_accuracy: 0.8237\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 18s 23ms/step - loss: 0.3308 - accuracy: 0.8578 - val_loss: 0.4047 - val_accuracy: 0.8149\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 20s 25ms/step - loss: 0.3212 - accuracy: 0.8641 - val_loss: 0.3771 - val_accuracy: 0.8306\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 20s 25ms/step - loss: 0.3131 - accuracy: 0.8673 - val_loss: 0.3726 - val_accuracy: 0.8329\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b4e795efdc0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Out of curiosity, run for 10 more epochs\n",
        "model_rnn.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=10,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZY439p_unAUu",
        "outputId": "7a07ce28-5db5-4d4c-b29b-2c3f73102479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "782/782 [==============================] - 20s 26ms/step - loss: 0.3065 - accuracy: 0.8708 - val_loss: 0.4021 - val_accuracy: 0.8189\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 18s 23ms/step - loss: 0.3009 - accuracy: 0.8736 - val_loss: 0.3841 - val_accuracy: 0.8280\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 18s 23ms/step - loss: 0.2956 - accuracy: 0.8768 - val_loss: 0.3936 - val_accuracy: 0.8249\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 19s 25ms/step - loss: 0.2905 - accuracy: 0.8776 - val_loss: 0.3703 - val_accuracy: 0.8373\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 18s 24ms/step - loss: 0.2870 - accuracy: 0.8804 - val_loss: 0.3618 - val_accuracy: 0.8408\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 22s 28ms/step - loss: 0.2830 - accuracy: 0.8829 - val_loss: 0.3649 - val_accuracy: 0.8401\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.2790 - accuracy: 0.8842 - val_loss: 0.3669 - val_accuracy: 0.8419\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 20s 26ms/step - loss: 0.2770 - accuracy: 0.8853 - val_loss: 0.3642 - val_accuracy: 0.8423\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 21s 27ms/step - loss: 0.2743 - accuracy: 0.8868 - val_loss: 0.3555 - val_accuracy: 0.8436\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.2719 - accuracy: 0.8879 - val_loss: 0.3613 - val_accuracy: 0.8446\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b4e76a142e0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JJRRrVZRnB0Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}